<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.3"/>


    <meta name="description" content="machine learning | python | ai | math" />



  <meta name="keywords" content="lfm,matrix factorization,nmf,pmf,svd," />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.3" />



<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    analytics: {
      google: ''
    },
    sidebar: 'post'
  };
</script>




  <title> 推荐系统中的矩阵分解技术 // Jerry's BLOG </title>
</head>

<body>
<!--[if lte IE 8]> <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'> <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode"><img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820" alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari." style='margin-left:auto;margin-right:auto;display: block;'/></a></div> <![endif]-->
  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <div id="header" class="header">
      <div class="header-inner">
        <h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand">
      <span class="logo">
        <i class="icon-logo"></i>
      </span>
      <span class="site-title">Jerry's BLOG</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>


  <ul id="menu" class="menu">
     
    
      
      <li class="menu-item menu-item-home">
        <a href="/">
          <i class="menu-item-icon icon-home"></i> <br />
          首页
        </a>
      </li>
    
      
      <li class="menu-item menu-item-categories">
        <a href="/categories">
          <i class="menu-item-icon icon-categories"></i> <br />
          分类
        </a>
      </li>
    
      
      <li class="menu-item menu-item-archives">
        <a href="/archives">
          <i class="menu-item-icon icon-archives"></i> <br />
          归档
        </a>
      </li>
    
      
      <li class="menu-item menu-item-tags">
        <a href="/tags">
          <i class="menu-item-icon icon-tags"></i> <br />
          标签
        </a>
      </li>
    
  </ul>


      </div>
    </div>

    <div id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content">
          

  <div id="posts" class="posts-expand">
    

  <div class="post post-type-normal ">
    <div class="post-header">

      
      
        <h1 class="post-title">
          
          
            
              推荐系统中的矩阵分解技术
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于 2015-10-09
        </span>

        
          <span class="post-category">
            &nbsp; | &nbsp; 分类于
            
              <a href="/categories/recommender-system/">recommender system</a>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
            &nbsp; | &nbsp;
            <a href="/2015/10/09/recommender-system/recommender-system-4-matrix-factorization/#comments" >
              <span class="post-comments-count ds-thread-count" data-thread-key="2015/10/09/recommender-system/recommender-system-4-matrix-factorization/"></span>
            </a>
          </span>
          
        
      </div>
    </div>

    <div class="post-body">

      
      

      
        <p>矩阵因子分解（matrix factorization）技术起源于线性代数，主要思路就是将一个矩阵成几个矩阵的乘积的形式。现在在人工智能的很多领域都有广泛应用，而且矩阵因子分解有很多变种，本文就来总结一下这些方法。</p>
<a id="more"></a>
<h2 id="矩阵能表达什么？">矩阵能表达什么？</h2><p>要用到矩阵因子分解技术，首先得这个问题得表达成矩阵的形式，有哪些问题可以表达成矩阵的形式呢？下面是最常见的一些：</p>
<ul>
<li>线性方程组</li>
<li>用户评分矩阵</li>
<li>图像</li>
<li>图论</li>
</ul>
<h3 id="线性方程组">线性方程组</h3><p>矩阵因子分解技术最开始的用途就是求解线性方程组。一个线性方程组可以表示成矩阵的形式如下：<br>$$<br>Ax = b<br>$$</p>
<p>其中A就是系数矩阵，直接求解上面的式子可能不太容易，因此可以对系数矩阵A进行分解，例如LU分解，就是将一个矩阵A分解为一个下三角矩阵L和一个上三角矩阵U的乘积。这样上式可以改写为：<br>$$<br>L(Ux) = b\<br>Ux = L^{-1}b<br>$$</p>
<p>求解三角阵是相对容易的，通过迭代就可以求解了。</p>
<h3 id="用户评分矩阵">用户评分矩阵</h3><p>在推荐系统中，一类重要的信息就是用户评分矩阵，这是一张很大的表，记录用户对物品的评分，只有很少的项是已知的，传统的以评分预测为目的的推荐系统就是要填补这个评分表。通过矩阵因子分解可以实现对评分矩阵的低秩近似，对于推荐系统来说，矩阵因子分解技术是一类很重要的方法。后面会逐一分析。</p>
<h3 id="图像">图像</h3><p>在计算机的眼中，图像本身就是一个矩阵，矩阵中的每一项就对应了图像中的一个像素点的值。因此在图像中矩阵分解技术也有广泛的应用。有很多矩阵因子分解技术就是从图像领域中诞生的，例如NMF。</p>
<h3 id="图论">图论</h3><p>在图论中，可以邻接矩阵的方式来表示一个图，因此很多图论算法都是基于邻接矩阵的，但是矩阵分解在图论中的应用倒是没怎么见识过。</p>
<h2 id="矩阵因子分解方法">矩阵因子分解方法</h2><p>主要介绍在推荐系统中，最常用的一些矩阵因子分解方法。</p>
<h3 id="SVD">SVD</h3><h4 id="特征值，特征向量与特征值分解">特征值，特征向量与特征值分解</h4><p>说到SVD，必须先了解特征值和特征向量。网上有很多关于特征值和特征向量的解释，包括数学的角度、物理的角度、几何的角度等等。简单来说，特征向量就是一组标准正交基，这些标准正交基构成了一个新的空间，特征值的模则代表矩阵在每个基上的投影长度。特征值越大，说明矩阵在对应的特征向量上的方差越大，功率越大，信息量越多。特征值分解的形式如下：<br>$$<br>A = Q\Sigma Q^{-1}<br>$$<br>通过特征值分解，可以实现降维，数据压缩，过滤噪声，特征提取等效果。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。</p>
<h4 id="奇异值，奇异向量与奇异值分解">奇异值，奇异向量与奇异值分解</h4><p>特征值分解是一个提取矩阵特征很不错的方法，但是它只适用于方阵。而在现实的世界中，我们看到的大部分矩阵都不是方阵，我们怎样才能像描述特征值一样描述这样一般矩阵呢的重要特征呢？奇异值分解就是用来干这个事的，奇异值分解是一个能适用于任意的矩阵的一种分解的方法。可以将SVD看作是特征值分解的扩展。<br>$$<br>A = U\Sigma V^{T}<br>$$<br>那么奇异值和特征值是怎么对应起来的呢？首先，我们将一个矩阵A的转置 * A，将会得到一个方阵，我们用这个方阵求特征值可以得到：<br>$$<br>(A^{T}A)v = \lambda v<br>$$<br>这里得到的v，就是我们上面的右奇异向量。此外我们还可以得到：<br>$$<br>\sigma = \sqrt \lambda \<br>u= \frac{Av}{\sigma}<br>$$<br>这里的σ就是上面说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前r大的奇异值来近似描述矩阵。</p>
<h4 id="SVD与PCA">SVD与PCA</h4><p>PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于我们用于机器学习的数据（主要是训练数据），方差大才有意义，不然输入的数据都是同一个点，那方差就为0了，这样输入的多个数据就等同于一个数据了。</p>
<p>PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。</p>
<h4 id="SVD在推荐系统中的应用">SVD在推荐系统中的应用</h4><p>前面也提到了，传统的推荐系统的任务就是补全评分矩阵。对于如何补全一个矩阵，历史上有很多的研究。一个空的矩阵有很多补全的方法，而我们要找的是一个对矩阵扰动最小的补全方法，那么什么是对矩阵扰动最小呢？一般认为，如果补全后的矩阵和原来矩阵的特征值相差不大就是扰动比较小。所以，最早的矩阵分解模型就是从数学上的SVD开始的。</p>
<p>SVD分解是早期推荐系统经常研究常用的矩阵分解方法，不过该方法具有以下缺点，因此很难在实际系统中使用。</p>
<ul>
<li>数据稀疏：一般现实中的评分矩阵是非常稀疏的，要应用SVD需要对数据进行补全，不同的补全方法会对结果造成影响。</li>
<li>计算复杂度高：该方法依赖的SVD分解方法的计算复杂度很高，特别是在稠密的大规模矩阵上更是非常慢，一般来说，这里的SVD分解用于1000维以上的矩阵就已经非常慢了，而实际中系统动辄就是上千万的用户和上百万的物品，所以这种方法无法使用。</li>
</ul>
<h3 id="Funk—SVD">Funk—SVD</h3><p>正是由于上面的两个缺点，SVD分解算法提出几年后在推荐系统领域没有得到广泛的关注，直到2006年Netflix Prize开始后，Simon Funk在博客上公开发表了一个算法（称为Funk-SVD），一下子引爆了整个学术界对矩阵分解类方法的关注。Simon Funk提出的矩阵分解方法后来被Netflix Prize 的冠军Koren称为Latent Factor Model(简称为LFM)。从矩阵分解的角度去说，如果我们将评分矩阵R分解为两个低维矩阵相乘：,那么对于用户u对物品i的评分的预测值，可以通过如下公式进行计算：，那么Simon Funk的思想很简单：可以直接通过训练集中的观察值利用最小化RMSE学习P，Q矩阵。Simon Funk 认为，既然我们用RMSE作为评测指标，那么如果能找到合适的PQ来最小化训练集中的预测误差，那么应该也能最小化测试集的预测误差。<br>$$<br>\min<em>{q^<em>, p^</em>}{\sum{(r</em>{ui}-q_i^Tp_u)^2}+\lambda (||q_i||^2+||p_u||^2)}<br>$$<br>后面加上了正则项，防止过拟合，当然也可以加上基准预测，使结果更准确。</p>
<p>如何对上式进行优化，是这个算法的关键，如果同时优化p和q，问题变成非凸的了，有两种常见的方法：</p>
<ul>
<li>交替最小二乘法（ALS）：先固定p，这时上式可以看作是关于q的二次函数，采用最小二乘法就能求解。然后固定q，来优化p。由于这两个步骤不相互依赖，这种方法非常并行化，在spark上就是采用的这种优化方法。</li>
<li>随机梯度下降法（SGD）：对训练数据中的每一项迭代优化，每次朝着梯度反方向修正参数。具体优化方法如下：<ul>
<li>$q<em>i \leftarrow q_i + \gamma(e</em>{ui} p_u-\lambda q_i) $</li>
<li>$p<em>u \leftarrow p_u + \gamma(e</em>{ui} q_i-\lambda p_u) $</li>
</ul>
</li>
</ul>
<p>其中$e<em>{ui}＝r</em>{ui}-\hat r_{ui}$表示误差，$\gamma$是学习速率。</p>
<h3 id="SVD++">SVD++</h3><p>可以看到，SVD中并没有利用好用户的隐式反馈信息，以看电影为例，如果一个用户评价了一个电影，这代表无论评分高低，在看电影之前这些电影对他来说是有吸引力的，更一般的，如果你有用户查看过电影介绍的数据，同样也可以加以利用，SVD++就是加入了这些信息。</p>
<p>为了达到这个目的，SVD++增加了第二个物品因子集合，即为每一个物品i关联一个因子向量$y<em>i\in R^f$。这些新的物品因子向量根据用户评分的物品集合来描述用户的特征。模型如下：<br>$$<br>\hat r</em>{ui} = q<em>i^T(p_u + |R(u)|^{-1/2}\sum</em>{j \in R(u)}y_i)<br>$$</p>
<p>其中，集合R(u)表示用户u评分的所有物品。优化的方法也类似：</p>
<ul>
<li>$q<em>i \leftarrow q_i + \gamma(e</em>{ui} (p<em>u+|R(u)|^{-1/2}\sum</em>{j \in R(u)}y_i)-\lambda q_i) $</li>
<li>$p<em>u \leftarrow p_u + \gamma(e</em>{ui} q_i-\lambda p_u) $</li>
<li>$\forall j \in R(u):y<em>i \leftarrow y_i+\gamma(e</em>{ui}|R(u)|^{-1/2}q_i-\lambda y_j)$</li>
</ul>
<h3 id="NMF">NMF</h3><p>1999年D.Lee和Seung发表在NATURE上的一篇文章“Learning the parts of objects by non-negativematrix factorization”提出了NMF的方法。通过论文的题目可以知道，NMF是限制了分解后的隐因子都是非负，这在也是符合现实场景的。现实中的很多值就是非负的，正因为有了非负的限制，才导致了NMF的结果只能由每个子部分相加得到，这是一个非常好的特性。这种特征与PCA（SVD）形成了鲜明的对比。后者就是通过全局的线性组合来重构一个矩阵。正是由于这个特点，NMF一般用于ICA（Independent Components Analysis),ICA在人脸识别和语义topic分析中比较成功。下面这张图片能很好的说明这个问题：</p>
<p><img src="http://img.my.csdn.net/uploads/201302/12/1360639032_1407.jpg" alt=""></p>
<p>关于VQ可以参考这篇文章<a href="http://blog.pluskid.org/?p=57" target="_blank" rel="external">漫谈 Clustering (番外篇): Vector Quantization</a></p>
<p>假设原矩阵为V，基矩阵为W，系数矩阵为H，希望用WH的乘积来拟合V，$V\approx WH$，噪声矩阵为E：<br>$$<br>E=V-WH<br>$$<br>NMF假定V是通过WH增加泊松噪声后产生的，可表示如下：<br>$$<br>P(V) = \frac{(WH)^V}{V!}\bullet e^{-WH}<br>$$<br>对上式求对数最大似然可得：<br>$$<br>lnP(V) = V\ln (WH)-\ln V!-WH \<br>\approx V\ln (WH)-V\ln V-WH<br>$$<br>求上式的最大值等价于：<br>$$<br>\max V\ln (WH)-WH<br>$$<br>写成求和的形式，就与论文中的一样了。<br>$$<br>\max \sum<em>i\sum_j[V</em>{ij}\log (WH)<em>{ij}-(WH)</em>{ij}]<br>$$<br>更多细节可以参考<a href="http://blog.csdn.net/acdreamers/article/details/44663421" target="_blank" rel="external">这篇文章</a>,然后就可以用下面的方法对上式迭代求解了。<br><img src="http://img.my.csdn.net/uploads/201302/12/1360639032_4539.jpg" alt=""></p>
<h3 id="PMF">PMF</h3><p>前面也提到过了，传统的SVD方法，在现实中应用的时候经常会遇到稀疏矩阵的问题。PMF正是为了解决这一问题提出来的。PMF假设评分服从如下高斯分布：<br>$$<br>p(R|U,V,\sigma^2)=\prod<em>{i=1}^N\prod</em>{j=1}^M[N(R<em>{ij}|U_i^TV_j,\sigma^2]^{I</em>{ij}}<br>$$<br>还假设U和V隐因子也服从0均值的高斯分布：<br>$$<br>p(U|\sigma<em>U^2)=\prod</em>{i=1}^NN(U<em>i|0,\sigma_U^2I) \<br>p(V|\sigma_V^2)=\prod</em>{j=1}^NN(V<em>j|0,\sigma_V^2I)<br>$$<br>根据贝叶斯公式：<br>$$<br>p(U,V|R,\sigma^2,\sigma_V^2,\sigma_U^2) \propto p(R|U,V,\sigma^2)\times p(U|\sigma_U^2) \times p(V|\sigma_V^2)<br>$$<br>将各式带入，然后求对数，可以发现最大后验概率等价于最小化下面的式子：<br>$$<br>L=\frac{1}{2}\sum</em>{i=1}^N\sum<em>{j=1}^MI</em>{ij}(R<em>{ij}-U_i^TV_j)^2+\frac{\lambda_U}{2}\sum</em>{i=1}^N||U<em>i||^2+\frac{\lambda_V}{2}\sum</em>{j=1}^M||V_j||^2<br>$$<br>上式的形式和Funk-SVD基本相同，可以看作是SVD的概率版本，当所有的评分都已知的时候，上式退化成SVD的目标函数。</p>

      
    </div>

    <div class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/lfm/"> #lfm </a>
          
            <a href="/tags/matrix-factorization/"> #matrix factorization </a>
          
            <a href="/tags/nmf/"> #nmf </a>
          
            <a href="/tags/pmf/"> #pmf </a>
          
            <a href="/tags/svd/"> #svd </a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2015/11/05/recommender-system/listwise-cf-1/">Listwise Collaboration Filtering</a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/09/25/recommender-system/recommender-system-3-datamining/">推荐系统三——推荐系统中的数据挖掘方法</a>
            
          </div>
        </div>
      

      
      
    </div>
  </div>



    
      <div class="comments" id="comments">
        
          <div class="ds-thread" data-thread-key="2015/10/09/recommender-system/recommender-system-4-matrix-factorization/"
               data-title="推荐系统中的矩阵分解技术" data-url="/2015/10/09/recommender-system/recommender-system-4-matrix-factorization/">
          </div>
        
      </div>
    
  </div>


        </div>

        
      </div>


      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <div id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview">
        <div class="site-author motion-element">
          <img class="site-author-image" src="/avatar.jpg" alt="Jerry" />
          <p class="site-author-name">Jerry</p>
        </div>
        <p class="site-description motion-element">machine learning | python | ai | math</p>
        <div class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">57</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">18</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">61</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </div>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

      </div>

      
        <div class="post-toc-wrap sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#矩阵能表达什么？"><span class="nav-number">1.</span> <span class="nav-text">矩阵能表达什么？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性方程组"><span class="nav-number">1.1.</span> <span class="nav-text">线性方程组</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用户评分矩阵"><span class="nav-number">1.2.</span> <span class="nav-text">用户评分矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#图像"><span class="nav-number">1.3.</span> <span class="nav-text">图像</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#图论"><span class="nav-number">1.4.</span> <span class="nav-text">图论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#矩阵因子分解方法"><span class="nav-number">2.</span> <span class="nav-text">矩阵因子分解方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SVD"><span class="nav-number">2.1.</span> <span class="nav-text">SVD</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#特征值，特征向量与特征值分解"><span class="nav-number">2.1.1.</span> <span class="nav-text">特征值，特征向量与特征值分解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#奇异值，奇异向量与奇异值分解"><span class="nav-number">2.1.2.</span> <span class="nav-text">奇异值，奇异向量与奇异值分解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVD与PCA"><span class="nav-number">2.1.3.</span> <span class="nav-text">SVD与PCA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVD在推荐系统中的应用"><span class="nav-number">2.1.4.</span> <span class="nav-text">SVD在推荐系统中的应用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Funk—SVD"><span class="nav-number">2.2.</span> <span class="nav-text">Funk—SVD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVD++"><span class="nav-number">2.3.</span> <span class="nav-text">SVD++</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NMF"><span class="nav-number">2.4.</span> <span class="nav-text">NMF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PMF"><span class="nav-number">2.5.</span> <span class="nav-text">PMF</span></a></li></ol></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </div>
      

    </div>
  </div>


    </div>

    <div id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; &nbsp; 
  2016
  <span class="with-love">
    <i class="icon-heart"></i>
  </span>
  <span class="author">Jerry</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



      </div>
    </div>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js"></script>


  <script type="text/javascript" src="/js/helpers.js"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js" id="motion.global"></script>




  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var $sidebarInner = $('.sidebar-inner');
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.didShow', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;
          var self = this;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      $(indicator).velocity('stop').velocity({
        opacity: action === 'show' ? 0.4 : 0
      }, { duration: 100 });
    }

  });
</script>


  <script type="text/javascript" id="sidebar.nav">
    $(document).ready(function () {
      var html = $('html');

      $('.sidebar-nav li').on('click', function () {
        var item = $(this);
        var activeTabClassName = 'sidebar-nav-active';
        var activePanelClassName = 'sidebar-panel-active';
        if (item.hasClass(activeTabClassName)) {
          return;
        }

        var currentTarget = $('.' + activePanelClassName);
        var target = $('.' + item.data('target'));

        currentTarget.velocity('transition.slideUpOut', 200, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', 200)
            .addClass(activePanelClassName);
        });

        item.siblings().removeClass(activeTabClassName);
        item.addClass(activeTabClassName);
      });

      $('.post-toc a').on('click', function (e) {
        e.preventDefault();
        var offset = $(escapeSelector(this.getAttribute('href'))).offset().top;
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        });
      });

      // Expand sidebar on post detail page by default, when post has a toc.
      var $tocContent = $('.post-toc-content');
      if (isDesktop() && CONFIG.sidebar === 'post') {
        if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
          displaySidebar();
        }
      }
    });
  </script>




  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
    });
  </script>

  

  
  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"jw-ml"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  


  
  

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
