<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.3"/>


    <meta name="description" content="machine learning | python | ai | math" />



  <meta name="keywords" content="big data,spark," />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.3" />



<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    analytics: {
      google: ''
    },
    sidebar: 'post'
  };
</script>




  <title> spark系列三——编程指南 // Jerry's BLOG </title>
</head>

<body>
<!--[if lte IE 8]> <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'> <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode"><img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820" alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari." style='margin-left:auto;margin-right:auto;display: block;'/></a></div> <![endif]-->
  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <div id="header" class="header">
      <div class="header-inner">
        <h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand">
      <span class="logo">
        <i class="icon-logo"></i>
      </span>
      <span class="site-title">Jerry's BLOG</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>


  <ul id="menu" class="menu">
     
    
      
      <li class="menu-item menu-item-home">
        <a href="/">
          <i class="menu-item-icon icon-home"></i> <br />
          首页
        </a>
      </li>
    
      
      <li class="menu-item menu-item-categories">
        <a href="/categories">
          <i class="menu-item-icon icon-categories"></i> <br />
          分类
        </a>
      </li>
    
      
      <li class="menu-item menu-item-archives">
        <a href="/archives">
          <i class="menu-item-icon icon-archives"></i> <br />
          归档
        </a>
      </li>
    
      
      <li class="menu-item menu-item-tags">
        <a href="/tags">
          <i class="menu-item-icon icon-tags"></i> <br />
          标签
        </a>
      </li>
    
  </ul>


      </div>
    </div>

    <div id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content">
          

  <div id="posts" class="posts-expand">
    

  <div class="post post-type-normal ">
    <div class="post-header">

      
      
        <h1 class="post-title">
          
          
            
              spark系列三——编程指南
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于 2015-09-01
        </span>

        
          <span class="post-category">
            &nbsp; | &nbsp; 分类于
            
              <a href="/categories/spark/">spark</a>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
            &nbsp; | &nbsp;
            <a href="/2015/09/01/spark-3-programming-guide/#comments" >
              <span class="post-comments-count ds-thread-count" data-thread-key="2015/09/01/spark-3-programming-guide/"></span>
            </a>
          </span>
          
        
      </div>
    </div>

    <div class="post-body">

      
      

      
        <h2 id="总览">总览</h2><p>从高层来看，一个Spark应用包含一个driver程序。</p>
<p>Spark主要提供了两种抽象的内容，其一是RDD，其二是共享变量。理解了这两个内容就基本理解了Spark编程。</p>
<h2 id="连接Spark">连接Spark</h2><p>使用scala开发Spark应用时，需要注意scala版本和Spark版本的兼容性。Spark 1.4.1使用的是scala 2.10.x。</p>
<p>在scala代码的开头需要导入下面两个类。</p>
<pre><code>import org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.SparkContext</span>
import org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.SparkConf</span>
</code></pre><h2 id="初始化Spark">初始化Spark</h2><p>首先需要创建一个SparkContext对象，用来访问Spark集群。而创建SparkContext对象时，需要传入一个SparkConf对象，用来指定Spark集群的一些配置信息，常见的有如下一些：</p>
<ul>
<li>setAppName(appName)：应用程序名，用来显示而已。</li>
<li>setMaster(master)：master可以“local”，表示在本地运行，也可以是指向Spark，Mesos或YARN集群的URL。如果是指向集群的URL，为了避免在程序中硬编码，可以在提交任务时，在spark-submit后用“—master spark://master ip:7077”的方式指定。</li>
</ul>
<h2 id="Resilient_Distributed_Datasets_(RDDs)">Resilient Distributed Datasets (RDDs)</h2><p>RDD是Spark的核心内容，是一种抽象的数据格式，支持并行操作和容错。Spark中所有的操作都是在RDD上进行的。</p>
<h3 id="创建RDD">创建RDD</h3><p>有两种创建RDD的方式，通过一个现有的数据集合或者是从外部的文件系统中创建。</p>
<h4 id="现有集合">现有集合</h4><p>通过调用SparkContext的parallelize方法，可以从dirver程序现有的集合中创建RDD，集合中的数据会被拷贝到分布式的文件系统中去，用来支持并行计算。</p>
<pre><code>val data = <span class="function"><span class="title">Array</span><span class="params">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span></span>
val distData = sc.<span class="function"><span class="title">parallelize</span><span class="params">(data)</span></span>
</code></pre><p>parallelize方法可以在第二个参数中指定partitions的数量，Spark在集群中的每个机器上会对每个partition都起一个任务，因此实际上是指定了一个节点内部的并行程度，一般partition的数量与CPU的核数相同，可以高效利用计算资源。</p>
<h4 id="外部数据集">外部数据集</h4><p>Spark能从任何Hadoop支持的数据源中创建RDD，包括本地文件系统， HDFS, Cassandra, HBase, Amazon S3,等等。而且Spark支持text files, SequenceFiles,和其他的Hadoop InputFormat.</p>
<p>文本文件可以通过SparkContext的textFile方法创建RDD，传入textFile的是URI，可以是本地的文件路径(这里的本机路径好像指的是本地的hdfs路径，可以省略前面的”hdfs://user/username”)或者hdfs://, s3n://等等。</p>
<p>这里的URI可以是一个目录、文本文件或是压缩文件。</p>
<p>和parallelize方法相同，textFile方法可以通过第二个参数来指定partition的数量，默认是每个block一个partition（hdfs的block默认是64MB）。</p>
<p>除了文本文件，Spark还支持一些其他的文件格式：</p>
<ul>
<li>SparkContext.wholeTextFiles：读取目录下的所有文件，并以(filename, content)对的形式存储。而textFile是按行保存的。</li>
<li>sequenceFile[K, V]：K，V用来指定key和value的类型，而且都必须是hadoop的Writable接口的子类。Spark会自动将Int转换成IntWritable，String转换成Text。</li>
<li>SparkContext.hadoopRDD：对于其他类型的Hadoop支持的输入类型，可以使用该函数。</li>
</ul>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val inputFormatClass = classOf[SequenceFileInputFormat[Text,Text]]</span><br><span class="line"><span class="tag">var</span> hadoopRdd = sc.<span class="function"><span class="title">hadoopRDD</span><span class="params">(conf, inputFormatClass, classOf[Text], classOf[Text])</span></span></span><br></pre></td></tr></table></figure>
<h3 id="RDD操作">RDD操作</h3><p>RDD支持两种类型的操作：transformations和actions，其中transformations是将一个RDD转换成另一个RDD，这种操作并不真正求值，只是将该操作相关的信息以链条的形式保存起来，而actions是真正求值，这时候该链条上的所有操作都会依次执行，得到最后的结果，并返回给driver程序。</p>
<p>这种设计，使得Spark非常高效，因为不用保存中间的结果。但是会带来一个问题，就是每次执行action操作时，该操作链上的所有transformations都会被重复执行。为了避免这个问题，可以在需要的时候对RDD进行持久化。使用persist (or cache)可以在内存中持久化RDD。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"data.txt"</span>)</span></span></span><br><span class="line">val lineLengths = lines.<span class="function"><span class="title">map</span><span class="params">(s =&gt; s.length)</span></span></span><br><span class="line">val totalLength = lineLengths.<span class="function"><span class="title">reduce</span><span class="params">((a, b)</span></span> =&gt; <span class="tag">a</span> + b)</span><br><span class="line">```   </span><br><span class="line"></span><br><span class="line">上面代码第一行，并不会真正加载数据到内容中，lines仅仅是指向文件的一个指针。第二行的map也不会真正计算，到第三行计算才真正发生，这时Spark会将整个任务划分成很多task，然后分配这些task给集群中的机器，每个节点计算完后，会将结果返回给driver，driver综合所有节点的返回结果得到最终的计算结果。</span><br><span class="line"></span><br><span class="line">如果这时又对lineLengths进行了其他的action操作，那么Spark重新将data.txt加载到内存中来。因此最好是对lineLengths进行持久化，在reduce之前添加如下代码：</span><br></pre></td></tr></table></figure>
<p>lineLengths.persist()<br><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">这样在第一次计算发生时，会将lineLengths的结果保存在内存中。对于需要重复利用的数据，持久化会提高效率。</span><br><span class="line"></span><br><span class="line">### 传递函数</span><br><span class="line">Spark的大部分API都需要传递函数，有两种推荐的传递函数的方式：</span><br><span class="line"></span><br><span class="line">-<span class="ruby"> 匿名函数：即lambda表达式，适合写简短的函数。</span><br><span class="line"></span>-<span class="ruby"> 全局单例对象（scala中用object定义单例对象，用<span class="class"><span class="keyword">class</span>定义类）中的静态方法</span></span></span><br></pre></td></tr></table></figure></p>
<p>object MyFunctions {<br>  def func1(s: String): String = { … }<br>}</p>
<p>myRdd.map(MyFunctions.func1)<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果传递类中的方法，那么在实际调用时，会将整个对象实例传递给集群。</span><br></pre></td></tr></table></figure></p>
<p>class MyClass {<br>  def func1(s: String): String = { … }<br>  def doStuff(rdd: RDD[String]): RDD[String] = { rdd.map(func1) }<br>}<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在访问外部变量时，情况也是类似。</span><br></pre></td></tr></table></figure></p>
<p>class MyClass {<br>  val field = “Hello”<br>  def doStuff(rdd: RDD[String]): RDD[String] = { rdd.map(x =&gt; field + x) }<br>}<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">为了避免这个问题，简单的方法是将类变量放到函数内部。</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">### 理解闭包</span></span><br><span class="line">Spark最难的问题之一就是当代码在集群上执行的时候，理解变量和方法的作用范围和生命周期。</span><br></pre></td></tr></table></figure></p>
<p>var counter = 0<br>var rdd = sc.parallelize(data)</p>
<p>// Wrong: Don’t do this!!<br>rdd.foreach(x =&gt; counter += x)</p>
<p>println(“Counter value: “ + counter)<br><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">闭包指的是一个完整的函数，Spark在分配task给节点时，会将函数以闭包的形式打包，也就是将该函数要用到的所有资源封装在一起，构成一个可以独立执行的闭包。例如上面的代码，在执行foreach函数时，会将counter变量复制一份和函数一起传递给计算节点。这样每个节点中都会有一份独立counter变量的拷贝，并不能共享。虽然在计算节点上，counter被修改了，但是driver中的counter任然是<span class="number">0</span>。</span><br><span class="line"></span><br><span class="line">为了解决这个问题，Spark引入了Accumulator，更多细节请查阅相关资料。</span><br><span class="line"></span><br><span class="line">还有一个典型的情况就是输出。如果在一台机器，使用rdd.foreach(println)会得到所有的输出，不会出现什么问题，但是如果在一个集群上，每个executor会将内容输出到本机的stdout中，而不会输出到driver的stdout中。因此，要先使用collect方法让driver得到所有的数据，但是这样可能会导致driver的内存溢出，因此当你只想查看部分结果时，最好使用<span class="keyword">take</span>方法： rdd.<span class="keyword">take</span>(<span class="number">100</span>).foreach(println)。</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">### Transformations</span></span><br><span class="line">|Transformation	| Meaning|</span><br><span class="line">|:---|:---|</span><br><span class="line">|map(func)	| <span class="keyword">Return</span> a <span class="keyword">new</span> distributed dataset formed <span class="keyword">by</span> passing <span class="keyword">each</span> element <span class="keyword">of</span> the source through a <span class="keyword">function</span> func.|</span><br><span class="line">|filter(func)	|<span class="keyword">Return</span> a <span class="keyword">new</span> dataset formed <span class="keyword">by</span> selecting those elements <span class="keyword">of</span> the source <span class="keyword">on</span> which func returns <span class="literal">true</span>.|</span><br><span class="line">|flatMap(func)	|Similar <span class="keyword">to</span> map, but <span class="keyword">each</span> input item can be mapped <span class="keyword">to</span> <span class="number">0</span> <span class="keyword">or</span> more output items (so func should <span class="keyword">return</span> a Seq rather than a <span class="built_in">single</span> item).|</span><br><span class="line">|mapPartitions(func)	|Similar <span class="keyword">to</span> map, but runs separately <span class="keyword">on</span> <span class="keyword">each</span> partition (block) <span class="keyword">of</span> the RDD, so func must be <span class="keyword">of</span> type Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; <span class="keyword">when</span> running <span class="keyword">on</span> an RDD <span class="keyword">of</span> type T.|</span><br><span class="line">|mapPartitionsWithIndex(func)	|Similar <span class="keyword">to</span> mapPartitions, but also provides func <span class="keyword">with</span> an <span class="built_in">integer</span> value representing the index <span class="keyword">of</span> the partition, so func must be <span class="keyword">of</span> type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; <span class="keyword">when</span> running <span class="keyword">on</span> an RDD <span class="keyword">of</span> type T.|</span><br><span class="line">|sample(withReplacement, fraction, seed)	|Sample a fraction fraction <span class="keyword">of</span> the data, <span class="keyword">with</span> <span class="keyword">or</span> without replacement, <span class="keyword">using</span> a given random number generator seed.|</span><br><span class="line">|union(otherDataset)	|<span class="keyword">Return</span> a <span class="keyword">new</span> dataset that contains the union <span class="keyword">of</span> the elements <span class="keyword">in</span> the source dataset <span class="keyword">and</span> the argument.|</span><br><span class="line">|intersection(otherDataset)	|<span class="keyword">Return</span> a <span class="keyword">new</span> RDD that contains the intersection <span class="keyword">of</span> elements <span class="keyword">in</span> the source dataset <span class="keyword">and</span> the argument.|</span><br><span class="line">|<span class="keyword">distinct</span>([numTasks]))	|<span class="keyword">Return</span> a <span class="keyword">new</span> dataset that contains the <span class="keyword">distinct</span> elements <span class="keyword">of</span> the source dataset.|</span><br><span class="line">|groupByKey([numTasks])	|<span class="keyword">When</span> called <span class="keyword">on</span> a dataset <span class="keyword">of</span> (K, V) pairs, returns a dataset <span class="keyword">of</span> (K, Iterable&lt;V&gt;) pairs. Note: <span class="keyword">If</span> you are grouping <span class="keyword">in</span> <span class="keyword">order</span> <span class="keyword">to</span> perform an aggregation (such <span class="keyword">as</span> a sum <span class="keyword">or</span> average) over <span class="keyword">each</span> <span class="keyword">key</span>, <span class="keyword">using</span> reduceByKey <span class="keyword">or</span> aggregateByKey will yield much better performance. Note: <span class="keyword">By</span> <span class="keyword">default</span>, the level <span class="keyword">of</span> parallelism <span class="keyword">in</span> the output depends <span class="keyword">on</span> the number <span class="keyword">of</span> partitions <span class="keyword">of</span> the parent RDD. You can pass an <span class="keyword">optional</span> numTasks argument <span class="keyword">to</span> <span class="keyword">set</span> a different number <span class="keyword">of</span> tasks.|</span><br><span class="line">|reduceByKey(func, [numTasks])	|<span class="keyword">When</span> called <span class="keyword">on</span> a dataset <span class="keyword">of</span> (K, V) pairs, returns a dataset <span class="keyword">of</span> (K, V) pairs <span class="keyword">where</span> the values <span class="keyword">for</span> <span class="keyword">each</span> <span class="keyword">key</span> are aggregated <span class="keyword">using</span> the given reduce <span class="keyword">function</span> func, which must be <span class="keyword">of</span> type (V,V) =&gt; V. <span class="keyword">Like</span> <span class="keyword">in</span> groupByKey, the number <span class="keyword">of</span> reduce tasks <span class="keyword">is</span> configurable through an <span class="keyword">optional</span> second argument.|</span><br><span class="line">|aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])	|<span class="keyword">When</span> called <span class="keyword">on</span> a dataset <span class="keyword">of</span> (K, V) pairs, returns a dataset <span class="keyword">of</span> (K, U) pairs <span class="keyword">where</span> the values <span class="keyword">for</span> <span class="keyword">each</span> <span class="keyword">key</span> are aggregated <span class="keyword">using</span> the given combine functions <span class="keyword">and</span> a neutral <span class="string">"zero"</span> value. Allows an aggregated value type that <span class="keyword">is</span> different than the input value type, <span class="keyword">while</span> avoiding unnecessary allocations. <span class="keyword">Like</span> <span class="keyword">in</span> groupByKey, the number <span class="keyword">of</span> reduce tasks <span class="keyword">is</span> configurable through an <span class="keyword">optional</span> second argument.|</span><br><span class="line">|sortByKey([ascending], [numTasks])	|<span class="keyword">When</span> called <span class="keyword">on</span> a dataset <span class="keyword">of</span> (K, V) pairs <span class="keyword">where</span> K <span class="keyword">implements</span> Ordered, returns a dataset <span class="keyword">of</span> (K, V) pairs sorted <span class="keyword">by</span> keys <span class="keyword">in</span> ascending <span class="keyword">or</span> descending <span class="keyword">order</span>, <span class="keyword">as</span> specified <span class="keyword">in</span> the <span class="built_in">boolean</span> ascending argument.|</span><br><span class="line">|<span class="keyword">join</span>(otherDataset, [numTasks])	|<span class="keyword">When</span> called <span class="keyword">on</span> datasets <span class="keyword">of</span> type (K, V) <span class="keyword">and</span> (K, W), returns a dataset <span class="keyword">of</span> (K, (V, W)) pairs <span class="keyword">with</span> all pairs <span class="keyword">of</span> elements <span class="keyword">for</span> <span class="keyword">each</span> <span class="keyword">key</span>. Outer joins are supported through leftOuterJoin, rightOuterJoin, <span class="keyword">and</span> fullOuterJoin.|</span><br><span class="line">|cogroup(otherDataset, [numTasks])	|<span class="keyword">When</span> called <span class="keyword">on</span> datasets <span class="keyword">of</span> type (K, V) <span class="keyword">and</span> (K, W), returns a dataset <span class="keyword">of</span> (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples. This operation <span class="keyword">is</span> also called groupWith.|</span><br><span class="line">|cartesian(otherDataset)	|<span class="keyword">When</span> called <span class="keyword">on</span> datasets <span class="keyword">of</span> types T <span class="keyword">and</span> U, returns a dataset <span class="keyword">of</span> (T, U) pairs (all pairs <span class="keyword">of</span> elements).|</span><br><span class="line">|pipe(command, [envVars])	|Pipe <span class="keyword">each</span> partition <span class="keyword">of</span> the RDD through a shell command, e.g. a Perl <span class="keyword">or</span> bash script. RDD elements are written <span class="keyword">to</span> the process<span class="comment">'s stdin and lines output to its stdout are returned as an RDD of strings.|</span></span><br><span class="line">|coalesce(numPartitions)	|Decrease the number <span class="keyword">of</span> partitions <span class="keyword">in</span> the RDD <span class="keyword">to</span> numPartitions. Useful <span class="keyword">for</span> running operations more efficiently after filtering down a large dataset.|</span><br><span class="line">|repartition(numPartitions)	|Reshuffle the data <span class="keyword">in</span> the RDD randomly <span class="keyword">to</span> create either more <span class="keyword">or</span> fewer partitions <span class="keyword">and</span> balance it across them. This always shuffles all data over the network.|</span><br><span class="line">|repartitionAndSortWithinPartitions(partitioner)	|Repartition the RDD according <span class="keyword">to</span> the given partitioner <span class="keyword">and</span>, within <span class="keyword">each</span> resulting partition, sort records <span class="keyword">by</span> their keys. This <span class="keyword">is</span> more efficient than calling repartition <span class="keyword">and</span> <span class="keyword">then</span> sorting within <span class="keyword">each</span> partition because it can push the sorting down <span class="keyword">into</span> the shuffle machinery.|</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">### Actions</span></span><br><span class="line">|Action	|Meaning|</span><br><span class="line">|:---|:---|</span><br><span class="line">|reduce(func)	|<span class="keyword">Aggregate</span> the elements <span class="keyword">of</span> the dataset <span class="keyword">using</span> a <span class="keyword">function</span> func (which takes two arguments <span class="keyword">and</span> returns one). The <span class="keyword">function</span> should be commutative <span class="keyword">and</span> associative so that it can be computed correctly <span class="keyword">in</span> parallel.|</span><br><span class="line">|collect()	|<span class="keyword">Return</span> all the elements <span class="keyword">of</span> the dataset <span class="keyword">as</span> an array at the driver program. This <span class="keyword">is</span> usually useful after a filter <span class="keyword">or</span> other operation that returns a sufficiently small subset <span class="keyword">of</span> the data.|</span><br><span class="line">|count()	|<span class="keyword">Return</span> the number <span class="keyword">of</span> elements <span class="keyword">in</span> the dataset.|</span><br><span class="line">|first()	|<span class="keyword">Return</span> the first element <span class="keyword">of</span> the dataset (similar <span class="keyword">to</span> <span class="keyword">take</span>(<span class="number">1</span>)).|</span><br><span class="line">|<span class="keyword">take</span>(n)	|<span class="keyword">Return</span> an array <span class="keyword">with</span> the first n elements <span class="keyword">of</span> the dataset.|</span><br><span class="line">|takeSample(withReplacement, num, [seed])	|<span class="keyword">Return</span> an array <span class="keyword">with</span> a random sample <span class="keyword">of</span> num elements <span class="keyword">of</span> the dataset, <span class="keyword">with</span> <span class="keyword">or</span> without replacement, optionally pre-specifying a random number generator seed.|</span><br><span class="line">|takeOrdered(n, [ordering])	|<span class="keyword">Return</span> the first n elements <span class="keyword">of</span> the RDD <span class="keyword">using</span> either their natural <span class="keyword">order</span> <span class="keyword">or</span> a <span class="keyword">custom</span> comparator.|</span><br><span class="line">|saveAsTextFile(path)	|Write the elements <span class="keyword">of</span> the dataset <span class="keyword">as</span> a <span class="keyword">text</span> file (<span class="keyword">or</span> <span class="keyword">set</span> <span class="keyword">of</span> <span class="keyword">text</span> files) <span class="keyword">in</span> a given directory <span class="keyword">in</span> the local filesystem, HDFS <span class="keyword">or</span> any other Hadoop-supported file system. Spark will <span class="keyword">call</span> toString <span class="keyword">on</span> <span class="keyword">each</span> element <span class="keyword">to</span> convert it <span class="keyword">to</span> a line <span class="keyword">of</span> <span class="keyword">text</span> <span class="keyword">in</span> the file.|</span><br><span class="line">|saveAsSequenceFile(path) </span><br><span class="line">(Java <span class="keyword">and</span> Scala)	|Write the elements <span class="keyword">of</span> the dataset <span class="keyword">as</span> a Hadoop SequenceFile <span class="keyword">in</span> a given path <span class="keyword">in</span> the local filesystem, HDFS <span class="keyword">or</span> any other Hadoop-supported file system. This <span class="keyword">is</span> available <span class="keyword">on</span> RDDs <span class="keyword">of</span> <span class="keyword">key</span>-value pairs that implement Hadoop<span class="comment">'s Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).|</span></span><br><span class="line">|saveAsObjectFile(path) </span><br><span class="line">(Java <span class="keyword">and</span> Scala)	|Write the elements <span class="keyword">of</span> the dataset <span class="keyword">in</span> a simple format <span class="keyword">using</span> Java serialization, which can <span class="keyword">then</span> be loaded <span class="keyword">using</span> SparkContext.objectFile().|</span><br><span class="line">|countByKey()	|Only available <span class="keyword">on</span> RDDs <span class="keyword">of</span> type (K, V). Returns a hashmap <span class="keyword">of</span> (K, Int) pairs <span class="keyword">with</span> the count <span class="keyword">of</span> <span class="keyword">each</span> <span class="keyword">key</span>.|</span><br><span class="line">|foreach(func)	|Run a <span class="keyword">function</span> func <span class="keyword">on</span> <span class="keyword">each</span> element <span class="keyword">of</span> the dataset. This <span class="keyword">is</span> usually done <span class="keyword">for</span> side effects such <span class="keyword">as</span> updating an Accumulator <span class="keyword">or</span> interacting <span class="keyword">with</span> external storage systems. Note: modifying variables other than Accumulators outside <span class="keyword">of</span> the foreach() may result <span class="keyword">in</span> undefined behavior. See Understanding closures <span class="keyword">for</span> more details.|</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">### Shuffle</span></span><br><span class="line">有些操作会自动调用Shuffle过程，例如repartition和coalesce, ‘ByKey类型的操作(除了counting)，像groupByKey，reduceByKey, 和<span class="keyword">join</span>操作，像cogroup，<span class="keyword">join</span>.</span><br><span class="line"></span><br><span class="line">Shuffle会导致节点间大量的数据传输，因此当数据量很大时尽量不要使用这类操作。</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">### RDD持久化</span></span><br><span class="line">Spark最主要的特点之一就是基于内存计算，因此在适当的时候，对RDD进行持久化是很必要的，一般可以提高<span class="number">10</span>倍以上的效率,而且Spark的cache有容错机制，如果cache中的RDD数据出现丢失，Spark会自动重新计算该RDD。</span><br><span class="line"></span><br><span class="line">Spark的持久化有不同的存储等级，常用的方法有persist和cache，其中cache使用默认的等级：StorageLevel.MEMORY_ONLY（只保存在内存中）。可以通过给persist传递参数的方法指定其他等级的持久化。</span><br><span class="line"></span><br><span class="line">|Storage Level	|Meaning|</span><br><span class="line">|:---|:---|</span><br><span class="line">|MEMORY_ONLY	|Store RDD <span class="keyword">as</span> deserialized Java objects <span class="keyword">in</span> the JVM. <span class="keyword">If</span> the RDD does <span class="keyword">not</span> fit <span class="keyword">in</span> memory, some partitions will <span class="keyword">not</span> be cached <span class="keyword">and</span> will be recomputed <span class="keyword">on</span> the fly <span class="keyword">each</span> time they<span class="comment">'re needed. This is the default level.|</span></span><br><span class="line">|MEMORY_AND_DISK	|Store RDD <span class="keyword">as</span> deserialized Java objects <span class="keyword">in</span> the JVM. <span class="keyword">If</span> the RDD does <span class="keyword">not</span> fit <span class="keyword">in</span> memory, store the partitions that don<span class="comment">'t fit on disk, and read them from there when they're needed.|</span></span><br><span class="line">|MEMORY_ONLY_SER	|Store RDD <span class="keyword">as</span> serialized Java objects (one <span class="built_in">byte</span> array per partition). This <span class="keyword">is</span> generally more space-efficient than deserialized objects, especially <span class="keyword">when</span> <span class="keyword">using</span> a fast serializer, but more CPU-intensive <span class="keyword">to</span> read.|</span><br><span class="line">|MEMORY_AND_DISK_SER	|Similar <span class="keyword">to</span> MEMORY_ONLY_SER, but spill partitions that don<span class="comment">'t fit in memory to disk instead of recomputing them on the fly each time they're needed.|</span></span><br><span class="line">|DISK_ONLY	|Store the RDD partitions only <span class="keyword">on</span> disk.|</span><br><span class="line">|MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.	|Same <span class="keyword">as</span> the levels above, but replicate <span class="keyword">each</span> partition <span class="keyword">on</span> two cluster nodes.|</span><br><span class="line">|OFF_HEAP (experimental)	|Store RDD <span class="keyword">in</span> serialized format <span class="keyword">in</span> Tachyon. Compared <span class="keyword">to</span> MEMORY_ONLY_SER, OFF_HEAP reduces garbage collection overhead <span class="keyword">and</span> allows executors <span class="keyword">to</span> be smaller <span class="keyword">and</span> <span class="keyword">to</span> share a pool <span class="keyword">of</span> memory, making it attractive <span class="keyword">in</span> environments <span class="keyword">with</span> large heaps <span class="keyword">or</span> multiple concurrent applications. Furthermore, <span class="keyword">as</span> the RDDs reside <span class="keyword">in</span> Tachyon, the crash <span class="keyword">of</span> an executor does <span class="keyword">not</span> lead <span class="keyword">to</span> losing the <span class="keyword">in</span>-memory cache. <span class="keyword">In</span> this mode, the memory <span class="keyword">in</span> Tachyon <span class="keyword">is</span> discardable. Thus, Tachyon does <span class="keyword">not</span> attempt <span class="keyword">to</span> reconstruct a block that it evicts <span class="keyword">from</span> memory. <span class="keyword">If</span> you plan <span class="keyword">to</span> use Tachyon <span class="keyword">as</span> the <span class="keyword">off</span> heap store, Spark <span class="keyword">is</span> compatible <span class="keyword">with</span> Tachyon out-<span class="keyword">of</span>-the-box. Please refer <span class="keyword">to</span> this page <span class="keyword">for</span> the suggested version pairings.|</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">## 共享变量</span></span><br><span class="line"><span class="preprocessor">### Broadcast</span></span><br><span class="line">Broadcast变量让每个节点上保存一个只读变量，而不是将该变量的拷贝传递给节点。Spark会使用高效的广播算法来传递Broadcast变量。因此当节点都需要访问同样的数据，而且在数据集很大时，使用Broadcast变量会提高效率。</span><br></pre></td></tr></table></figure></p>
<p>val broadcastVar = sc.broadcast(Array(1, 2, 3))<br>broadcastVar.value<br><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="preprocessor">### Accumulators</span></span><br><span class="line">就像前面也提到的一样，Accumulators主要用来实现想counter、<span class="keyword">sum</span>这种需要节点间通信的支持并行的变量。</span><br><span class="line"></span><br><span class="line">如果为Accumulators指定了名字，那么可以在Spark UI上看到。</span><br></pre></td></tr></table></figure></p>
<p>scala&gt; val accum = sc.accumulator(0, “My Accumulator”)<br>accum: spark.Accumulator[Int] = 0</p>
<p>scala&gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum += x)<br>…<br>10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s</p>
<p>scala&gt; accum.value<br>res2: Int = 10<br>```</p>

      
    </div>

    <div class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/big-data/"> #big data </a>
          
            <a href="/tags/spark/"> #spark </a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/07/28/scala-2/">scala编程二—-</a>
            
          </div>
        </div>
      

      
      
    </div>
  </div>



    
      <div class="comments" id="comments">
        
          <div class="ds-thread" data-thread-key="2015/09/01/spark-3-programming-guide/"
               data-title="spark系列三——编程指南" data-url="/2015/09/01/spark-3-programming-guide/">
          </div>
        
      </div>
    
  </div>


        </div>

        
      </div>


      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <div id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview">
        <div class="site-author motion-element">
          <img class="site-author-image" src="/avatar.jpg" alt="Jerry" />
          <p class="site-author-name">Jerry</p>
        </div>
        <p class="site-description motion-element">machine learning | python | ai | math</p>
        <div class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">18</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">9</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">29</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </div>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

      </div>

      
        <div class="post-toc-wrap sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#总览"><span class="nav-number">1.</span> <span class="nav-text">总览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#连接Spark"><span class="nav-number">2.</span> <span class="nav-text">连接Spark</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#初始化Spark"><span class="nav-number">3.</span> <span class="nav-text">初始化Spark</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Resilient_Distributed_Datasets_(RDDs)"><span class="nav-number">4.</span> <span class="nav-text">Resilient Distributed Datasets (RDDs)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#创建RDD"><span class="nav-number">4.1.</span> <span class="nav-text">创建RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#现有集合"><span class="nav-number">4.1.1.</span> <span class="nav-text">现有集合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#外部数据集"><span class="nav-number">4.1.2.</span> <span class="nav-text">外部数据集</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD操作"><span class="nav-number">4.2.</span> <span class="nav-text">RDD操作</span></a></li></ol></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </div>
      

    </div>
  </div>


    </div>

    <div id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; &nbsp; 
  2015
  <span class="with-love">
    <i class="icon-heart"></i>
  </span>
  <span class="author">Jerry</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



      </div>
    </div>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js"></script>


  <script type="text/javascript" src="/js/helpers.js"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js" id="motion.global"></script>




  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var $sidebarInner = $('.sidebar-inner');
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.didShow', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;
          var self = this;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      $(indicator).velocity('stop').velocity({
        opacity: action === 'show' ? 0.4 : 0
      }, { duration: 100 });
    }

  });
</script>


  <script type="text/javascript" id="sidebar.nav">
    $(document).ready(function () {
      var html = $('html');

      $('.sidebar-nav li').on('click', function () {
        var item = $(this);
        var activeTabClassName = 'sidebar-nav-active';
        var activePanelClassName = 'sidebar-panel-active';
        if (item.hasClass(activeTabClassName)) {
          return;
        }

        var currentTarget = $('.' + activePanelClassName);
        var target = $('.' + item.data('target'));

        currentTarget.velocity('transition.slideUpOut', 200, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', 200)
            .addClass(activePanelClassName);
        });

        item.siblings().removeClass(activeTabClassName);
        item.addClass(activeTabClassName);
      });

      $('.post-toc a').on('click', function (e) {
        e.preventDefault();
        var offset = $(escapeSelector(this.getAttribute('href'))).offset().top;
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        });
      });

      // Expand sidebar on post detail page by default, when post has a toc.
      var $tocContent = $('.post-toc-content');
      if (isDesktop() && CONFIG.sidebar === 'post') {
        if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
          displaySidebar();
        }
      }
    });
  </script>




  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
    });
  </script>

  

  
  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"jw-ml"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  


  
  

</body>
</html>
